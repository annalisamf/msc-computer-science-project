import pandas as pdimport pickleimport spacyfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_scorenlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])spacy.load('en_vectors_web_lg', vocab=nlp.vocab)nlp.max_length = 1500000stopwords = nlp.Defaults.stop_words | {'re', 'rt', 'co', 'amp'}# this function creates a dataset with the tweets of a selected list of usersdef joined_df(list_of_users, path_tweets):    li = []    for user in list_of_users:        timeline = pd.read_csv(path_tweets + "/%s_tweets.csv" % user,                               index_col=None, header=0)        timeline = timeline.dropna(subset=['text'])  # some text/tweets are blank        li.append(timeline)    return pd.concat(li, axis=0, ignore_index=True)# this function joins timelines of single users and selects only name and textdef group_tweets(labelled_df, path_tweets):    # create dataframe with tweets of labelled users(joined) and group the timeline in one row per user    grouped_df = joined_df(labelled_df.screen_name.tolist(), path_tweets).groupby('screen_name')['text'].apply(        lambda x: '. '.join(x)).reset_index()    # merging the names/labels with the tweets, and keeping only relevant columns (text and labels)    grouped_df = pd.merge(grouped_df, labelled_df, on='screen_name')[['text', 'label']]    return grouped_df# fit the model and print the score reportdef fit_predict_report(model, X_train, y_train, X_test, y_test):    model.fit(X_train, y_train)    train_predict = model.predict(X_train)    test_predict = model.predict(X_test)    print("Confusion Matrix")    print(confusion_matrix(y_test, test_predict))    print("Classification Report")    print(classification_report(y_test, test_predict))    print(f'Accuracy on training set {accuracy_score(y_train, train_predict)}')    print(f'Accuracy on test set {accuracy_score(y_test, test_predict)}')def most_informative_feature_for_class_svm(vectorizer, classifier, n=10):    labelid = 0  # this is the coef we're interested in.    feature_names = vectorizer.get_feature_names()    svm_coef = classifier.coef_    topn = sorted(zip(svm_coef[labelid], feature_names))[-n:]    for coef, feat in topn:        print(feat, coef)def show_most_informative_features(vectorizer, clf, n=20):    feature_names = vectorizer.get_feature_names()    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))    top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])    for (coef_1, fn_1), (coef_2, fn_2) in top:        print("\t%.4f\t%-15s\t\t%.4f\t%-15s" % (coef_1, fn_1, coef_2, fn_2))def most_informative_feature_for_class(vectorizer, classifier, classlabel, n=10):    labelid = list(classifier.classes_).index(classlabel)    feature_names = vectorizer.get_feature_names()    topn = reversed(sorted(zip(classifier.coef_[labelid], feature_names))[-n:])    for coef, feat in topn:        print(classlabel, feat, coef)def lemmatization(list_of_docs):    keep_tags = ['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']    lemmatized_text = []    for sent in list_of_docs:        doc = nlp(sent)        lemmatized_text.append(            [token.lemma_.lower() for token in doc if             token.pos_ in keep_tags and token.is_alpha and len(                 token.lemma_) > 1 and token.lemma_.strip().lower() not in stopwords])        #     is_apha removes numbers, punctuations and urls        print(f"{len(lemmatized_text)} sentences lemmatized")    return lemmatized_text# creates a datframe with lemmatized timelines and saves itdef df_lemmatized(users, path_tweets, save_lemma_df):    age_df = joined_df(users, path_tweets).groupby('screen_name')['text'].apply(        lambda x: '. '.join(x)).reset_index()    age_df['text'] = [" ".join(word) for word in lemmatization(age_df.text.values.tolist())]    pickle.dump(age_df, open('/Users/annalisa/PycharmProjects/MSc_Project/age/age_data'                             '/%s' % save_lemma_df, 'wb'), protocol=4)    return age_df# applies the model to predict the age to a set of users# returns a dataframe with the username, tweets and predicted agedef age_df_prediction(model, vectorizer, age_df, save_predict_df):    vectorized_features = vectorizer.fit_transform(age_df.text).toarray()    predict = model.predict(vectorized_features)    age_df['predicted'] = predict    pickle.dump(age_df, open('/Users/annalisa/PycharmProjects/MSc_Project/age/age_data'                             '/%s' % save_predict_df, 'wb'), protocol=4)    return age_df